# 推荐算法，图像识别及测试

目前状况：不易对算法处理后的数据的正确性进行校验，
    
    一方面是由于数据量太大，几亿的数据量；
    另一方面是由于输出结果的不确定性，
    一般而言，大数据运用在推荐、预测、数据挖掘、机器学习等领域。

可选的方法有:

内部检测：
   
    1. 走读代码，检查数据过滤和数据转化是否合理，有无不必要的
    data lost
    2. 检查是否有不符合字段正则的值`比如ip、时间、邮箱、mac地址的正则等，甚至是否存在乱码等)`
    3. 检查结果和具体的产品情况是否符合
    `比如活跃小时段： *一般来说大部分产品凌晨用户不太活跃；*年龄分布；学历比例；操作系统（安卓 ios pc)`
    4. 数据互斥 `消费能力和年龄互斥：比如学生一般小于25，但消费能力不随年纪小而减小`

抽样

    从所有的结果当中进行抽样，然后进行基本的业务测试，看是否符合设计需求。或者采用众包测试。当然如果是几亿数据量的话，测试的覆盖率的确会比较低。
    
构造数据
    
    自己构造测试数据，然后对这些数据的输出结果进行测试
    
AB测试

    如果对于迭代的产品或者算法，我们可以用之前的数据进行测试，然后将结果进行一个基于数据的diff测试。新的测试结果，至少应该不比之前的糟糕。但如果是一个新产品的话，这个就无法实现了。

算法接口测试
    
    将算法当成一个有结果返回的接口，，所以我们可以做接口的单元测试和冒烟测试，还可以进行压测，即在预估的 qps 下看 rt 是否满足业务方的要求，load 是否过大，超时和错误的比例是否符合一定的预期等。
    

对于大数据领域来说，测试输入无法穷举，对结果来说，系统输出y一直在'理想的好'和'极值的差'之间游离。
这类系统验收与传统质量保障方案有较大差异。

    针对算法特性，依然可以分为：
        算法功能性验证： 达到目标，选取数据集，推荐符合百分之多少的预期, 异常测试，容错，抗压，死锁，健壮性 
        非功能性验证：  算法复杂度O(N)，性能拐点, 是否资源损耗，计算密集型还是存储密集型
        同类算法的交错验证: 比较不同算法的准确度，或者AB测试与自己的上一次进行对比
    
推荐系统的流程
    
    召回 --> 打分排序 --> 输出
        1. 召回阶段通常的手段是协同过滤比较场景，也有使用 embedding 的方式通过向量之间的距离进行召回。假如现在推荐一个商品，那么就得基于用户感兴趣的物品 。而这些感兴趣的物品其实就是用户有过历史行为的物品，例如最近一段时间内的点击、加购、收藏、购买的物品。将这些商品做为 trigger 进行召回，按照协同过滤算法算出商品之间的相似分值，然后按照一定数量进行过滤。因为过滤也是依靠分数来进行的，所以这一步也称粗排。
        2. 召回完商品后，我们需要对这些商品进行精排，也就是用模型来预估 ctr。一般情况下 LR、GBDT、FM 用的比较多，深度网络相对用的少。主要为了考虑到性能，尤其是 rt，因为绝大部分的精排都是需要实时预测的，所以对耗时有一定的要求。
        3. 模型预测的步骤如下：首先针对召回的商品进行特征的补充，例如该商品的一级类目、叶子类目（一级类目代表比较，叶子类目代表最细分的类目）、被多少用户购买等；然后再加入人的特征，例如性别、年龄、收入、对类目的偏好等；最后将这些物品和用户的信息做为 feature，根据模型预测的结果进行排序输出。
    打分过程中的模型需要提前训练和部署。其中训练集包括label和feature，label 是用户的历史行为（点击、购买等），feature 是用户的特征（性别、年龄）和商品的特征（类别、价格）。
    
算法端质量

    算法端的质量可以分为3个方面：算法数据、算法模型、算法效果；
    算法数据： 算法在训练前的数据处理非常重要。数据的来源，特征的构造，数据抽取、加工整个的过程都有可能会出现错误，而且数据一般都是存储在分布式系统数据库里，因此需要借助类似 hive 这样的工具将 sql 转换成 MapReduce 的任务去进行离线的计算，离线任务的产出通常会耗费不少的时间，而对于一些日更新的模型通过对数据对产出时间有一定的要求。
    因此数据这块最主要的保证点为：数据本身的质量，和数据的产出时间。数据本身的质量一般可以通过数据大小的整体抖动，以及关键字段是否为空、主键是否重复等，具体实现方法包括简单 sql 或者 udf ，借助工程能力就能做到预警、检查、出报表等。
    算法模型： 模型的本身在迭代过程中也是需要关注的，不过通常算法同学的训练优化也是参考这些指标，所以我们也可以把这几个指标做为模型本身好坏的评估，主要包括准确率、召回率、AUC。
    算法效果： 判断这个算法推荐出的效果究竟好不好，是一个非常主观的事情，但是我们仍然要衡量算法的好坏。

指标评估：
    
    指标化推荐结果，也就是将推荐的结果用不同的指标来进行说明，通过这些指标，我们可以更加全面了解推荐系统，部分指标不一定越高越好，需要让它保持在一定的范围内。下面我们看下这些指标
   
覆盖率
    
    定义：推荐系统能够推荐出来的 “商品 / 类目” 占“总商品 / 类目”集合的比例。假设系统的用户集合为 U，推荐系统给每个用户推荐一个长度为 N 的物品列表 R(u) ，总物品为 N。那么：
    意义：描述推荐系统对物品发掘的能力。
    举个例子，网站的条目千千万万，推荐系统能否保证让新的一些文章有足够的机会曝光出去呢？还是有些商品永远都无法得到推荐曝光的机会。这个指标反应的就是这个情况，显然物品的覆盖率是达不到 100% 的，但是我们可以看类目的覆盖率来进行衡量，假设全网所有的一级大类目一共 2 千个（和全网上亿的物品相比非常的少），那么推荐系统一天之内推荐出去的商品对应的一级类目，这个就是我们要衡量的标准。如果一级类目的覆盖率达不到 100%，那么肯定是有问题的。

基尼系数

    覆盖率反应出的分布情况是比较有限的，我们只能知道哪些类目覆盖了，哪些没有覆盖，那类目之间究竟哪个类目占的多，哪个类目占的少呢？为了更细致地描述推荐系统发掘长尾的能力，我们需要统计推荐列表中不同类目出现次数的分布，引入基尼系数来评价。
    定义：按照类目的流行度（曝光次数）从大到小排序后进行统计后进行洛伦茨曲线的绘制。
    做法：以类目分布基尼系数为例，算出所有的类目被曝光的次数，需要以天周期为单位进行数据的统计。
    这里需要说明一下，基尼系数越大代表所有类目的分布越不均匀，系数越小代表类目分布越均匀。我们知道，每个电商网站都有其侧重的类目，因此绝对平均不是一件好事，头部的类目占比稍多一些但是不能太离谱，举个例子 100 个类目，前 5 个占比到 30～40% 是相对比较好的。当然绝对的只看这个数据意义也不是很大，更多的是长期对这个指标进行监控，看是否会发生大的变动。

打散度

    定义：描述推荐结果中结果数据的分散程度。
    这里需要解释一下，这里首先是对两两物品（不同的位置）计算为打散度后，得出整体的打散度。相似函数 sim 代表两两是否相同，相同则为 1，不相似则为 0。关于两个内容之间距离对打散度的影响，不能是线性的关系，因为随着两个商品出现的位置越来越大，用户对重复商品的感受会逐渐的减弱（很近的位置就有两个相似的内容觉得会有些重复，但是如果比较远的位置有两个相似的一般是可以接受的），一般双列流屏幕出现内容大概是 4 个，0.85^(5-1) 大概在 0.5 左右，所以如果是 5 以内，则打散度会很低，但是如果 > 5 了，打散度就不会衰减的比较厉害了。 相似的两个物品越靠近，权重越大。

更新率

    定义：描述推荐系统不断迭代过程中推荐结果变化程度的指标。
    上面公式还是以类目为例，S昨日S 昨日代表昨天一天出现的所有商品所在的类目的个数，然后两天的交集除以并集，计算得出推荐出商品所属类目的更新率。
    
发现性

    定义：推荐系统对用户未产生过关系的商品的发现能力。
    在全网商品中，可能有一些比较好的商品，但是用户从来都没有点击过类似的物品，这时候推荐系统推荐给用户的时候，用户很有可能会眼前一亮，满满惊喜。
    同样以类目为例，今天我点击了一个我感兴趣的商品，而这个商品的类似恰恰是我前一周都没有点击过的内容，这就说明推荐系统的为我推荐了一个我之前都没有关注过并且我感兴趣的内容，也就是系统的发现性，在算出每个人的值之后，再进行求平均计算。

上新率

    定义： 新内容被推荐系统推荐的曝光情况，这里可以从两个维度产出这项指标。
    意义：对于一些社区类产品 UGC 内容的推荐，用户生产的优质是整个社区最重要的一部分，及时的曝光用户的新内容对于增加用户留存和给社区增添活力都有很大的帮助，因此需要这两个指标来评估推荐算法对于新内容的推荐能力。

NDCG
    
    有些文章中推荐使用这个指标，但是个人觉得这个更加适合评价搜索的结果。

失效率
    
    定义： 表示系统没有推荐或推荐后未被用户点击数据占全集的比例。
    S(0) 表示实际点击次数为 0 的数据个数；S 表示推荐集合的总数。
    首先需要定义一个时间范围来计算没有被推荐出的。其含义为最终未被用户真正感知的数据的占比，未感知包含未推荐和推荐出去后未被点击的内容。

健壮性

    定义：算法健壮性的评测主要利用模拟攻击。首先，给定一个数据集和一个算法，可以用这个算法给这个数据集中的用户生成推荐列表。然后，用常用的攻击方法向数据集中注入噪声数据，然后利用算法在注入噪声后的数据集上再次给用户生成推荐列表。最后，通过比较攻击前后推荐列表的相似度评测算法的健壮性。
    总结：适合在离线环境进行完成，针对模型本身的评测。
    除了上面介绍的通过这些指标的方法来进行评估，当推荐真正运用在业务上，通过业务侧的一些数据反馈也可以知道推荐算法的好坏。具体看下面两项：

负反馈

    定义：负反馈相当于一个轻量级便携的用户反馈，用户可以直接对推荐出的内容给与反馈，推荐系统在拿到了用户实时反馈后就会立刻针对反馈信息对推荐结果做出相应的调整，而我们也可以在事后拿到负反馈的整体数据来评价推荐系统在用户侧是否有重大舆情产生。一般 app 的推荐都会有负反馈机制，如图：

CTR (Click Through Rate)

    Click-Through-Rate，即点击率，点击数 / 曝光数。推荐算法效果的最最重要指标，没有之一。一般算法好不好，都会直接用这个指标直接定义。通常算法模型在迭代的过程中都会进行 A/B test。所谓 ab test 就是有一个基准桶，一个对比桶，通过收集两个不同方案在用户侧的点击率，来评估算法的好坏，一般来说当流量特别大的时候，基本上一个 ab 实验上线几分钟就可以出算法的好坏了。当然算法的分桶不仅限只有两个桶，像下面这个推荐每个分桶的数据都可以非常直观的展示出来。一般需要借助Blink 等工具来实时的显示点击率数据。

CVR (Click Value Rate)

    Click Value Rate，即转化率，转化数 / 点击数。通常在广告上用的比较多，对于商品来说也就是用户最终点击并且购买的转化率。因为最终决定转化的因素还是比较多的，不单单是推荐算法影响的，所以这个指标通常不做为模型迭代优化的衡量标准，但是由于其和最终的 "钱" 挂钩，所以一般领导会更加关注这个指标。

总结：
    
    我们需要真正的将这些内容结合到业务上去，看它究竟反应业务什么样的情况，抽丝剥茧，更加的理解业务、反哺业务。任何一个指标都需要对业务有指导意义，真正帮助业务提升。整体的质量方案的概要图如下。
    
`Ref: https://blog.csdn.net/sinat_26811377/article/details/99785903`